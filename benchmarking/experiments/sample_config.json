{
    "providers": [
        "vLLM"
    ],
    "models": [
        "common-model"
    ],
    "num_requests": 100,
    "input_tokens": 10,
    "streaming": true,
    "max_output": 100,
    "verbose": true,
    "backend": true
}
